"""–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø–æ—Ç–æ–∫–æ–≤–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –∞—Ä—Ö–∏–≤–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ 1–° –ø–æ—Ä—Ü–∏—è–º–∏."""

import asyncio
import sys
import time
import warnings
import argparse
from pathlib import Path
from typing import List, Generator
from concurrent.futures import ThreadPoolExecutor
import concurrent.futures

# –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –æ—Ç –≤–Ω–µ—à–Ω–∏—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫
warnings.filterwarnings("ignore", category=FutureWarning, module="soupsieve")

sys.path.insert(0, str(Path(__file__).parent))

from src.core.config import settings
from src.core.elasticsearch import es_client
from src.parsers.hbk_parser import HBKParser
from src.parsers.indexer import indexer
from src.models.doc_models import Documentation, ParsedHBK, HBKEntry, HBKFile


class StreamingIndexer:
    """–ü–æ—Ç–æ–∫–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä —Å –ø–∞—Ä—Å–∏–Ω–≥–æ–º –∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–µ–π –ø–æ—Ä—Ü–∏—è–º–∏."""
    
    def __init__(self, batch_size: int = 500, max_documents: int = None, max_workers: int = 4):
        self.batch_size = batch_size
        self.max_documents = max_documents
        self.max_workers = max_workers
        self.parser = HBKParser()
        
    def _initialize_extractor(self, archive_path: Path):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤."""
        # –ü—Ä–æ—Å—Ç–æ –≤—ã–∑—ã–≤–∞–µ–º –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–µ—Ä–∞ –∫–æ—Ç–æ—Ä—ã–π —É–∂–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç _zip_command –∏ _archive_path
        all_entries = self.parser._extract_external_7z(archive_path)
        
        if not self.parser._zip_command:
            raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å 7zip")
            
        return all_entries
        
    def _chunk_list(self, lst: List, chunk_size: int) -> Generator[List, None, None]:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –Ω–∞ —á–∞—Å—Ç–∏ –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞."""
        for i in range(0, len(lst), chunk_size):
            yield lst[i:i + chunk_size]
    
    def _extract_files_batch(self, archive_path: Path, file_entries: List[HBKEntry]) -> dict:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–∞–∫–µ—Ç —Ñ–∞–π–ª–æ–≤ –∏–∑ –∞—Ä—Ö–∏–≤–∞ —á–µ—Ä–µ–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª-—Å–ø–∏—Å–æ–∫."""
        from src.core.utils import safe_subprocess_run, create_safe_temp_dir, safe_remove_dir
        import tempfile
        
        temp_dir = create_safe_temp_dir("batch_extract_")
        extracted_contents = {}
        
        try:
            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —Å–æ —Å–ø–∏—Å–∫–æ–º —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
            with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt', encoding='utf-8') as list_file:
                for entry in file_entries:
                    list_file.write(f"{entry.path}\n")
                list_file_path = list_file.name
            
            try:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ–∞–π–ª—ã –∏—Å–ø–æ–ª—å–∑—É—è —Ñ–∞–π–ª-—Å–ø–∏—Å–æ–∫
                cmd = [self.parser._zip_command, 'e', str(archive_path), f'-i@{list_file_path}', f'-o{temp_dir}', '-y']
                result = safe_subprocess_run(cmd, timeout=120)
                
                if result.returncode == 0:
                    # –ß–∏—Ç–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≤—Å–µ—Ö –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
                    for entry in file_entries:
                        # –ò—â–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π —Ñ–∞–π–ª (7zip –º–æ–∂–µ—Ç –∏–∑–º–µ–Ω–∏—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–∞–ø–æ–∫)
                        file_name = Path(entry.path).name
                        extracted_files = list(temp_dir.rglob(file_name))
                        
                        for extracted_file in extracted_files:
                            if extracted_file.is_file():
                                try:
                                    with open(extracted_file, 'rb') as f:
                                        extracted_contents[entry.path] = f.read()
                                    break
                                except Exception as e:
                                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {entry.path}: {e}")
                
                return extracted_contents
                
            finally:
                # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª-—Å–ø–∏—Å–æ–∫
                try:
                    Path(list_file_path).unlink()
                except:
                    pass
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –±–∞—Ç—á–µ–≤–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è: {e}")
            return {}
        finally:
            safe_remove_dir(temp_dir)
    
    def _parse_single_file(self, entry: HBKEntry, hbk_file_path: Path) -> List[Documentation]:
        """–ü–∞—Ä—Å–∏—Ç –æ–¥–∏–Ω HTML —Ñ–∞–π–ª –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."""
        try:
            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
            temp_result = ParsedHBK(
                file_info=self._create_file_info(hbk_file_path)
            )
            
            # –ü–∞—Ä—Å–∏–º –æ–¥–∏–Ω HTML —Ñ–∞–π–ª (–º–µ—Ç–æ–¥ –¥–æ–±–∞–≤–ª—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ temp_result.documentation)
            self.parser._create_document_from_html(entry, temp_result)
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
            return temp_result.documentation if temp_result.documentation else []
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ñ–∞–π–ª–∞ {entry.path}: {e}")
            return []
    
    async def _parse_files_parallel(self, batch_entries: List[HBKEntry], extracted_contents: dict, hbk_file_path: Path, max_workers: int = 4) -> tuple:
        """–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –ø–∞—Ä—Å–∏—Ç —Ñ–∞–π–ª—ã –≤ –ø–æ—Ä—Ü–∏–∏."""
        loop = asyncio.get_event_loop()
        batch_docs = []
        parsed_count = 0
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        valid_entries = []
        
        for entry in batch_entries:
            if entry.path in extracted_contents:
                # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞
                entry.content = extracted_contents[entry.path]
                valid_entries.append(entry)
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –≤ –ø—É–ª–µ –ø–æ—Ç–æ–∫–æ–≤
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞
            tasks = [
                loop.run_in_executor(executor, self._parse_single_file, entry, hbk_file_path)
                for entry in valid_entries
            ]
            
            # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö –∑–∞–¥–∞—á
            try:
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                for i, result in enumerate(results):
                    if isinstance(result, Exception):
                        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ñ–∞–π–ª–∞ {valid_entries[i].path}: {result}")
                    elif result:  # result - —ç—Ç–æ —Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                        batch_docs.extend(result)
                        parsed_count += 1
                        
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏: {e}")
        
        return batch_docs, parsed_count
    
    def _create_file_info(self, file_path: Path) -> HBKFile:
        """–°–æ–∑–¥–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ HBK —Ñ–∞–π–ª–µ."""
        return HBKFile(
            path=str(file_path),
            size=file_path.stat().st_size,
            modified=file_path.stat().st_mtime
        )
    
    def _filter_html_entries(self, entries: List[HBKEntry]) -> List[HBKEntry]:
        """–§–∏–ª—å—Ç—Ä—É–µ—Ç —Ç–æ–ª—å–∫–æ HTML —Ñ–∞–π–ª—ã –∏–∑ –∞—Ä—Ö–∏–≤–∞."""
        html_entries = []
        for entry in entries:
            if (not entry.is_dir and 
                entry.path.endswith('.html') and 
                'objects/' in entry.path.replace('\\', '/')):
                html_entries.append(entry)
        return html_entries
    
    async def stream_index_archive(self, hbk_file_path: str) -> bool:
        """–ü–æ—Ç–æ–∫–æ–≤–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è: –ø–∞—Ä—Å–∏–Ω–≥ –ø–æ—Ä—Ü–∏—è–º–∏ + –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∫–∞–∂–¥–æ–π –ø–æ—Ä—Ü–∏–∏."""
        print(f"=== –ü–æ—Ç–æ–∫–æ–≤–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è (–ø–∞—Ä—Å–∏–Ω–≥ –ø–æ—Ä—Ü–∏—è–º–∏ –ø–æ {self.batch_size}) ===")
        
        try:
            # –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ Elasticsearch
            print("üîå –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Elasticsearch...")
            connected = await es_client.connect()
            if not connected:
                print("‚ùå Elasticsearch –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
                return False
            
            print("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Elasticsearch —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
            
            # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–π –∏–Ω–¥–µ–∫—Å –∏ —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π
            print("üóëÔ∏è –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞...")
            if await es_client.index_exists():
                if es_client._client:
                    await es_client._client.indices.delete(index=es_client._config.index_name)
            
            print("üèóÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞...")
            await es_client.create_index()
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ –∏–∑ –∞—Ä—Ö–∏–≤–∞
            print(f"üìÅ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤ –∏–∑ –∞—Ä—Ö–∏–≤–∞: {Path(hbk_file_path).name}")
            start_extract_time = time.time()
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –∏ –ø–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –æ–¥–Ω–∏–º –≤—ã–∑–æ–≤–æ–º
            all_entries = self._initialize_extractor(Path(hbk_file_path))
            if not all_entries:
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –∏–∑ –∞—Ä—Ö–∏–≤–∞")
                return False
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ HTML —Ñ–∞–π–ª—ã
            html_entries = self._filter_html_entries(all_entries)
            extract_time = time.time() - start_extract_time
            
            total_html_files = len(html_entries)
            print(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {extract_time:.2f} —Å–µ–∫")
            print(f"üìÅ –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –≤ –∞—Ä—Ö–∏–≤–µ: {len(all_entries)}")
            print(f"üìÑ HTML —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {total_html_files}")
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –ª–∏–º–∏—Ç —Ñ–∞–π–ª–æ–≤ –µ—Å–ª–∏ –∑–∞–¥–∞–Ω
            if self.max_documents and total_html_files > self.max_documents:
                html_entries = html_entries[:self.max_documents]
                total_html_files = len(html_entries)
                print(f"‚ö†Ô∏è –û–≥—Ä–∞–Ω–∏—á–µ–Ω–æ –¥–æ {self.max_documents} HTML —Ñ–∞–π–ª–æ–≤")
            
            print(f"üì¶ –†–∞–∑–º–µ—Ä –ø–æ—Ä—Ü–∏–∏: {self.batch_size}")
            total_batches = (total_html_files + self.batch_size - 1) // self.batch_size
            print(f"üî¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ä—Ü–∏–π: {total_batches}")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª—ã –ø–æ—Ä—Ü–∏—è–º–∏
            print("üöÄ –ó–∞–ø—É—Å–∫ –ø–æ—Ç–æ–∫–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏...")
            start_process_time = time.time()
            
            total_indexed = 0
            batch_num = 0
            
            for batch_entries in self._chunk_list(html_entries, self.batch_size):
                batch_num += 1
                batch_start_time = time.time()
                
                print(f"\nüì¶ –ü–æ—Ä—Ü–∏—è {batch_num}/{total_batches} ({len(batch_entries)} —Ñ–∞–π–ª–æ–≤)")
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å—é –ø–æ—Ä—Ü–∏—é —Ñ–∞–π–ª–æ–≤ –æ–¥–Ω–æ–π –∫–æ–º–∞–Ω–¥–æ–π
                extract_start_time = time.time()
                extracted_contents = self._extract_files_batch(Path(hbk_file_path), batch_entries)
                extract_time = time.time() - extract_start_time
                
                print(f"   üìÅ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ: {len(extracted_contents)}/{len(batch_entries)} —Ñ–∞–π–ª–æ–≤, {extract_time:.2f}—Å")
                
                # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –ø–∞—Ä—Å–∏–º –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
                parse_start_time = time.time()
                batch_docs, parsed_count = await self._parse_files_parallel(
                    batch_entries, extracted_contents, Path(hbk_file_path), max_workers=self.max_workers
                )
                parse_time = time.time() - parse_start_time
                print(f"   üìù –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ ({self.max_workers} –ø–æ—Ç–æ–∫–æ–≤): {parsed_count}/{len(batch_entries)} —Ñ–∞–π–ª–æ–≤, {len(batch_docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, {parse_time:.2f}—Å")
                
                # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—É—é –ø–æ—Ä—Ü–∏—é
                if batch_docs:
                    index_start_time = time.time()
                    
                    batch_hbk = ParsedHBK(
                        file_info=self._create_file_info(Path(hbk_file_path)),
                        documentation=batch_docs
                    )
                    
                    success = await indexer.index_documentation(batch_hbk)
                    
                    if not success:
                        print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –ø–æ—Ä—Ü–∏–∏ {batch_num}")
                        return False
                    
                    index_time = time.time() - index_start_time
                    total_indexed += len(batch_docs)
                    
                    batch_total_time = time.time() - batch_start_time
                    docs_per_sec = len(batch_docs) / batch_total_time if batch_total_time > 0 else 0
                    
                    print(f"   üíæ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è: {len(batch_docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, {index_time:.2f}—Å")
                    print(f"   ‚úÖ –ü–æ—Ä—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {batch_total_time:.2f}—Å, {docs_per_sec:.1f} –¥–æ–∫/—Å")
                    print(f"   üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: {total_indexed} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –ø–æ—Ä—Ü–∏—è {batch_num}/{total_batches}")
                else:
                    print(f"   ‚ö†Ô∏è –ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –≤ –ø–æ—Ä—Ü–∏–∏ {batch_num}")
            
            process_time = time.time() - start_process_time
            total_time = extract_time + process_time
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Ç–æ–≥–æ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∏–Ω–¥–µ–∫—Å–µ
            print("\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Ç–æ–≥–æ–≤–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
            await es_client.refresh_index()
            indexed_docs_count = await es_client.get_documents_count()
            
            # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            print(f"\nüéâ –ü–û–¢–û–ö–û–í–ê–Ø –ò–ù–î–ï–ö–°–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê –£–°–ü–ï–®–ù–û!")
            print(f"üìä –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
            print(f"   ‚Ä¢ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤: {extract_time:.2f} —Å–µ–∫")
            print(f"   ‚Ä¢ –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ—Ä—Ü–∏—è–º–∏: {process_time:.2f} —Å–µ–∫")
            print(f"   ‚Ä¢ –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time:.2f} —Å–µ–∫")
            print(f"   ‚Ä¢ HTML —Ñ–∞–π–ª–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {total_html_files}")
            print(f"   ‚Ä¢ –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å–æ–∑–¥–∞–Ω–æ: {total_indexed}")
            print(f"   ‚Ä¢ –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∏–Ω–¥–µ–∫—Å–µ: {indexed_docs_count}")
            print(f"   ‚Ä¢ –†–∞–∑–º–µ—Ä –ø–æ—Ä—Ü–∏–∏: {self.batch_size}")
            print(f"   ‚Ä¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ä—Ü–∏–π: {batch_num}")
            if self.max_documents:
                print(f"   ‚Ä¢ –õ–∏–º–∏—Ç HTML —Ñ–∞–π–ª–æ–≤: {self.max_documents}")
            print(f"   ‚Ä¢ –°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å: {total_indexed/total_time:.1f} –¥–æ–∫/—Å–µ–∫")
            
            if total_indexed == indexed_docs_count:
                print("‚úÖ –í—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω—ã")
            else:
                print(f"‚ö†Ô∏è –ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ: —Å–æ–∑–¥–∞–Ω–æ {total_indexed}, –≤ –∏–Ω–¥–µ–∫—Å–µ {indexed_docs_count}")
            
            return total_indexed == indexed_docs_count
            
        except Exception as e:
            print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
            import traceback
            traceback.print_exc()
            return False
        finally:
            print("üîå –û—Ç–∫–ª—é—á–µ–Ω–∏–µ –æ—Ç Elasticsearch...")
            await es_client.disconnect()


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å –ø–∞—Ä—Å–∏–Ω–≥–æ–º –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤."""
    parser = argparse.ArgumentParser(description="–ü–æ—Ç–æ–∫–æ–≤–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∞—Ä—Ö–∏–≤–∞ 1–°")
    parser.add_argument(
        "--batch-size", 
        type=int, 
        default=500, 
        help="–†–∞–∑–º–µ—Ä –ø–æ—Ä—Ü–∏–∏ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 500)"
    )
    parser.add_argument(
        "--max-docs", 
        type=int, 
        default=None, 
        help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ HTML —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π)"
    )
    parser.add_argument(
        "--workers", 
        type=int, 
        default=4, 
        help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 4)"
    )
    
    args = parser.parse_args()
    
    # –ù–∞—Ö–æ–¥–∏–º .hbk —Ñ–∞–π–ª
    hbk_dir = Path(settings.data.hbk_directory)
    hbk_files = list(hbk_dir.glob("*.hbk"))
    
    if not hbk_files:
        print("‚ùå .hbk —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return False
    
    # –°–æ–∑–¥–∞–µ–º –ø–æ—Ç–æ–∫–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä
    streaming_indexer = StreamingIndexer(
        batch_size=args.batch_size, 
        max_documents=args.max_docs,
        max_workers=args.workers
    )
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é
    result = await streaming_indexer.stream_index_archive(str(hbk_files[0]))
    
    if result:
        print("\n‚úÖ –ü–æ—Ç–æ–∫–æ–≤–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
    else:
        print("\n‚ùå –ü–æ—Ç–æ–∫–æ–≤–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å —Å –æ—à–∏–±–∫–∞–º–∏!")
    
    return result


if __name__ == "__main__":
    asyncio.run(main())
